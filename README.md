# Project_SemEval_NLP

Detecting Machine-Generated Code with Multiple Programming Languages, Generators,
and Application Scenarios

**Project Overview**
This project addresses the challenge of distinguishing between human-written code and code generated by Large Language Models (LLMs). Developed for SemEval Task X, the system utilizes a CodeBERT-based architecture to not only detect machine-generated code but also attribute it to specific LLM families.

**Objectives**
The project focuses on two distinct subtasks:

Subtask A: Human vs. Machine Classification
Goal: Determine whether a code snippet is written by a human or generated by a machine.
Labeling: Binary labels where 0 represents human and 1 represents machine-generated.

Subtask B: LLM-Family Attribution
Goal: Identify the specific origin of the code from 11 possible categories (Human plus 10 popular LLM families such as DeepSeek-AI, Qwen, Meta-LLaMA, Mistral, and OpenAI).
Labeling: Multi-class identifier (0-10).

**Methodology**
Model Architecture
We utilize CodeBERT, a bimodal transformer model pre-trained on both natural language descriptions and source code. This allows the system to capture fine-grained stylistic details—such as variable naming, indentation, and comment styles—that reveal the code's origin.

The pipeline involves:
Input Encoding: Using Byte-Pair Encoding (BPE) tokenizer trained on multiple programming languages.
Contextual Representation: The [CLS] token embedding is used as a holistic representation of the code snippet.

Classification Heads:
Subtask A: A shallow binary classification head.
Subtask B: A softmax classifier with 11 output neurons.
