# -*- coding: utf-8 -*-
"""NLP Task-B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19lKlJywpdiNE1cFqUKJY83mJhG0phutk
"""

!pip install -U "transformers>=4.34.0" "datasets>=2.15.0" "accelerate>=0.23.0" \
    "pandas>=2.0.0" "numpy>=1.26.0" "scikit-learn>=1.3.0" \
    "joblib>=1.3.0" "pyarrow>=14.0.0" "fastparquet>=2023.8.0" "tqdm>=4.66.0"

from google.colab import drive
drive.mount('/content/drive')

from datasets import Dataset, load_dataset
from transformers import AutoTokenizer
import pandas as pd

def load_parquet_dataset(parquet_path, text_field="code", label_field="label"):
    """
    Load a parquet file as a streaming Hugging Face dataset.
    Returns an IterableDataset that can be sampled from.
    """
    dataset = load_dataset("parquet", data_files=parquet_path, split="train", streaming=True)
    return dataset.map(lambda x: {text_field: x[text_field], label_field: x[label_field]})


class CodeDatasetPreprocessor:
    """
    Tokenizes code or text fields in Hugging Face datasets for transformer models.
    """

    def __init__(self, model_name_or_path, max_length=512, text_field="code"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)
        self.max_length = max_length
        self.text_field = text_field

    def tokenize_batch(self, examples):
        return self.tokenizer(
            examples[self.text_field],
            truncation=True,
            padding="max_length",
            max_length=self.max_length
        )
    def prepare(self, dataset):
        """
        Apply tokenization to a Hugging Face Dataset (non-streaming).
        """
        return dataset.map(
            self.tokenize_batch,
            batched=True,
            remove_columns=[self.text_field]
        )

# src/metrics.py
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import math
from typing import Dict

def classification_metrics(preds, labels) -> Dict:
    """Compute accuracy, precision, recall, and f1 (weighted).
    preds can be logits (ndarray) or 1d predicted labels."""
    import numpy as _np
    preds_arr = _np.array(preds)
    if preds_arr.ndim > 1:
        pred_labels = preds_arr.argmax(axis=1)
    else:
        pred_labels = preds_arr
    acc = accuracy_score(labels, pred_labels)
    prec, rec, f1, _ = precision_recall_fscore_support(labels, pred_labels, average='weighted', zero_division=0)
    return {"accuracy": acc, "precision": prec, "recall": rec, "f1": f1}

def perplexity(loss):
    return math.exp(loss)

# src/models.py
from transformers import AutoModelForSequenceClassification, AutoConfig

def get_sequence_classification_model(name_or_path, num_labels, from_pretrained=True):
    if from_pretrained:
        model = AutoModelForSequenceClassification.from_pretrained(name_or_path, num_labels=num_labels)
    else:
        config = AutoConfig.from_pretrained(name_or_path, num_labels=num_labels)
        model = AutoModelForSequenceClassification.from_config(config)
    return model

# ==============================
# ‚úÖ SETUP
# ==============================
!pip install -q transformers datasets accelerate evaluate scikit-learn

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import torch, gc

# ==============================
# ‚úÖ DATA HANDLING
# ==============================
def load_parquet_dataset(parquet_path, text_field="code", label_field="label"):
    dataset = load_dataset("parquet", data_files=parquet_path)["train"]
    sample = dataset[0]
    if text_field not in sample:
        raise ValueError(f"'{text_field}' not found in dataset columns: {list(sample.keys())}")
    if label_field not in sample:
        raise ValueError(f"'{label_field}' not found in dataset columns: {list(sample.keys())}")
    return dataset


def subsample_dataset(dataset, max_size=12000, seed=42):
    """Keep manageable subset (for Colab GPU)."""
    if len(dataset) > max_size:
        dataset = dataset.shuffle(seed=seed).select(range(max_size))
    return dataset


# ==============================
# ‚úÖ TOKENIZATION
# ==============================
class CodeDatasetPreprocessor:
    def __init__(self, model_name_or_path, max_length=512, text_field="code"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)
        self.max_length = max_length
        self.text_field = text_field

    def tokenize_batch(self, examples):
        return self.tokenizer(
            examples[self.text_field],
            truncation=True,
            max_length=self.max_length,
        )

    def prepare(self, dataset):
        return dataset.map(self.tokenize_batch, batched=True, remove_columns=[self.text_field])


# ==============================
# ‚úÖ METRICS
# ==============================
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, preds)
    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average="weighted", zero_division=0)
    return {"accuracy": acc, "precision": prec, "recall": rec, "f1": f1}


# ==============================
# ‚úÖ MAIN TRAINING FUNCTION
# ==============================
def main():
    model_name = "microsoft/codebert-base"
    train_path = "/content/drive/MyDrive/trainingset.parquet"
    val_path = "/content/drive/MyDrive/validationset.parquet"
    out_dir = "/content/drive/MyDrive/experiments_codebert"
    num_labels = 11
    epochs = 15              # ‚¨ÜÔ∏è more epochs = better convergence
    batch_size = 8
    grad_accum = 8
    lr = 1.5e-5              # ‚úÖ slightly lower LR for stable fine-tuning
    os.makedirs(out_dir, exist_ok=True)
    print("üìÇ Loading parquet datasets...")
    train_ds = load_parquet_dataset(train_path)
    val_ds = load_parquet_dataset(val_path)

    print("üìâ Subsampling datasets (if large)...")
    train_ds = subsample_dataset(train_ds, max_size=12000)
    val_ds = subsample_dataset(val_ds, max_size=4000)

    print("üîß Tokenizing datasets...")
    preproc = CodeDatasetPreprocessor(model_name, max_length=512, text_field="code")
    train_tok = preproc.prepare(train_ds)
    val_tok = preproc.prepare(val_ds)

    print("ü§ñ Initializing model...")
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

    # Enable memory-efficient training
    model.gradient_checkpointing_enable()

    args = TrainingArguments(
        output_dir=out_dir,
        eval_strategy="epoch",
        save_strategy="epoch",
        learning_rate=lr,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=epochs,
        weight_decay=0.05,
        logging_steps=200,
        fp16=True,
        gradient_accumulation_steps=grad_accum,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        report_to="none",
        lr_scheduler_type="cosine",
        warmup_ratio=0.1,
        optim="adamw_torch",           # ‚úÖ native fused optimizer
        dataloader_num_workers=2,
        push_to_hub=False,
    )

    data_collator = DataCollatorWithPadding(tokenizer=preproc.tokenizer)

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_tok,
        eval_dataset=val_tok,
        tokenizer=preproc.tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    # üßπ Clear any unused memory
    gc.collect()
    torch.cuda.empty_cache()

    print("üöÄ Starting fine-tuning with CodeBERT...")
    trainer.train()

    trainer.save_model(out_dir)
    preproc.tokenizer.save_pretrained(out_dir)
    print(f"‚úÖ Training complete. Best model saved to {out_dir}")


if __name__ == "__main__":
    main()

# src/predict_binary.py
from transformers import logging
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer
from datasets import Dataset
import os

os.environ["WANDB_DISABLED"] = "true"
logging.set_verbosity_error()

def main():
    # --- Step 1: Paths ---
    model_dir = "/content/drive/MyDrive/experiments_codebert/checkpoint-1880"
    test_path = "/content/drive/MyDrive/test.parquet"
    output_csv = "/content/drive/MyDrive/taskb.csv"

    # --- Step 2: Load test data ---
    print(f"üìÇ Loading test data from {test_path} ...")
    test_df = pd.read_parquet(test_path)
    test_df = test_df.reset_index(drop=True)
    test_df["ID"] = test_df.index

    # --- Step 3: Load tokenizer and model ---
    print(f"üöÄ Loading model and tokenizer from {model_dir} ...")
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForSequenceClassification.from_pretrained(model_dir)

    # --- Step 4: Tokenize test data ---
    print("üî° Tokenizing test data ...")
    def preprocess(batch):
        return tokenizer(batch["code"], truncation=True, padding="max_length", max_length=256)
    test_ds = Dataset.from_pandas(test_df)
    test_tokenized = test_ds.map(preprocess, batched=True)

    # --- Step 5: Initialize Trainer for prediction ---
    trainer = Trainer(model=model)

    # --- Step 6: Predict ---
    print("ü§ñ Running predictions ...")
    preds = trainer.predict(test_tokenized)
    logits = preds.predictions
    y_pred = torch.argmax(torch.tensor(logits), dim=1).numpy()
    # --- Step 7: Save ONLY ID & predicted label ---
    submission = pd.DataFrame({
        "ID": test_df["ID"],
        "Label": y_pred
    })
    submission.to_csv(output_csv, index=False)
    print(f"‚úÖ Predictions saved to {output_csv} (shape: {submission.shape})")
if __name__ == "__main__":
    main()

# src/predict_binary.py
from transformers import logging
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer
from datasets import Dataset
import os

os.environ["WANDB_DISABLED"] = "true"
logging.set_verbosity_error()

def main():
    # --- Step 1: Paths ---
    model_dir = "/content/drive/MyDrive/experiments_codebert/checkpoint-1880"
    test_path = "/content/drive/MyDrive/Test.parquet"
    output_csv = "/content/drive/MyDrive/taskb.csv"

    # --- Step 2: Load test data ---
    print(f"üìÇ Loading test data from {test_path} ...")
    test_df = pd.read_parquet(test_path)
    test_df["ID"] = test_df.index

    # --- Step 3: Load tokenizer and model ---
    print(f"üöÄ Loading model and tokenizer from {model_dir} ...")
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForSequenceClassification.from_pretrained(model_dir)

    # --- Step 4: Tokenize test data ---
    print("üî° Tokenizing test data ...")
    def preprocess(batch):
        return tokenizer(batch["code"], truncation=True, padding="max_length", max_length=256)
    test_ds = Dataset.from_pandas(test_df)
    test_tokenized = test_ds.map(preprocess, batched=True)

    # --- Step 5: Initialize Trainer for prediction ---
    trainer = Trainer(model=model)

    # --- Step 6: Predict ---
    print("ü§ñ Running predictions ...")
    preds = trainer.predict(test_tokenized)
    logits = preds.predictions
    y_pred = torch.argmax(torch.tensor(logits), dim=1).numpy()
    # --- Step 7: Save ONLY ID & predicted label ---
    submission = pd.DataFrame({
        "ID": test_df["ID"],
        "Label": y_pred
    })
    submission.to_csv(output_csv, index=False)
    print(f"‚úÖ Predictions saved to {output_csv} (shape: {submission.shape})")
if __name__ == "__main__":
    main()